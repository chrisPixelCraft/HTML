{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26300,"status":"ok","timestamp":1703026674834,"user":{"displayName":"Rafael Hsieh","userId":"06462378086560153088"},"user_tz":-480},"id":"tIwgoXUBTBzn","outputId":"fe167af2-879e-48bb-baf4-751afc5978c7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting libsvm-official\n","  Downloading libsvm-official-3.32.0.tar.gz (39 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from libsvm-official) (1.11.4)\n","Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->libsvm-official) (1.23.5)\n","Building wheels for collected packages: libsvm-official\n","  Building wheel for libsvm-official (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for libsvm-official: filename=libsvm_official-3.32.0-cp310-cp310-linux_x86_64.whl size=123884 sha256=956bd52e414edf29696b8a06664c644e95b9466ba9e95fcdcdc366c93dfcc119\n","  Stored in directory: /root/.cache/pip/wheels/61/3b/1b/73bb4869517f96a26c82b47ccdb9ec48f12f4466de2371eff6\n","Successfully built libsvm-official\n","Installing collected packages: libsvm-official\n","Successfully installed libsvm-official-3.32.0\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.46.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"]}],"source":["!pip install -U libsvm-official\n","!pip install numpy matplotlib pandas"]},{"cell_type":"code","execution_count":108,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11346,"status":"ok","timestamp":1703038271919,"user":{"displayName":"Rafael Hsieh","userId":"06462378086560153088"},"user_tz":-480},"id":"kBGjTWA6TYOy","outputId":"ec318545-dab9-44b2-985d-5342c7fef467"},"outputs":[{"name":"stdout","output_type":"stream","text":["Eout:  8.735926305015353\n"]}],"source":["# p9\n","\n","from libsvm.svmutil import *\n","import numpy as np\n","\n","\n","def transform_X(x):\n","  X = np.array([0,0,0,0,0,0,0,0])\n","  # X = np.empty(shape = (len(x), 8))\n","  for i in range(len(x)):\n","    dic = x[i]\n","    rows = [value for key,value in dic.items()]\n","    X = np.vstack([X, rows])\n","\n","  X = np.delete(X,0,0)\n","  return X\n","\n","\n","def compute_squared_error(y_true):\n","    return np.mean((y_true - np.mean(y_true)) ** 2)\n","\n","\n","def find_best_split(data, target):\n","    # Ensure data is a 2D numpy array and target is a 1D numpy array\n","    if len(data.shape) != 2 or len(target.shape) != 1:\n","        raise ValueError(\"Data must be a 2D array and target must be a 1D array\")\n","\n","    n_features = data.shape[1]\n","    best_feature, best_theta, min_error = None, None, float('inf')\n","\n","    for feature in range(n_features):\n","        sorted_values = np.unique(np.sort(data[:, feature]))\n","        thetas = (sorted_values[:-1] + sorted_values[1:]) / 2\n","\n","        for theta in thetas:\n","            left_mask = data[:, feature] <= theta\n","            right_mask = ~left_mask\n","\n","            left_target = target[left_mask]\n","            right_target = target[right_mask]\n","\n","            left_error = compute_squared_error(left_target) if left_target.size > 0 else 0\n","            right_error = compute_squared_error(right_target) if right_target.size > 0 else 0\n","\n","            total_error = left_error * left_mask.sum() + right_error * right_mask.sum()\n","\n","            if total_error < min_error:\n","                best_feature, best_theta, min_error = feature, theta, total_error\n","\n","    return best_feature, best_theta\n","\n","\n","class Node:\n","    def __init__(self, feature_index=None, threshold=None, left=None, right=None, value=None):\n","        self.feature_index = feature_index\n","        self.threshold = threshold\n","        self.left = left\n","        self.right = right\n","        self.value = value\n","\n","class DecisionTreeRegressor:\n","    def __init__(self, depth = None):\n","        self.depth = depth\n","        self.tree = {}\n","\n","    def fit(self, data, target, depth=0):\n","      # self.root = self.fit(data, target)\n","        if depth == self.depth or data.shape[0] < 2:\n","            self.tree['value'] = target.mean()\n","            return\n","\n","        best_feature, best_theta = find_best_split(data, target)\n","        if best_feature is None:\n","            self.tree['value'] = target.mean()\n","            return\n","\n","        self.tree = {'feature': best_feature, 'theta': best_theta, 'left': {}, 'right': {}}\n","\n","        left_mask = data[:, best_feature] <= best_theta\n","        right_mask = ~left_mask\n","\n","        left_subtree = DecisionTreeRegressor(self.depth)\n","        left_subtree.fit(data[left_mask], target[left_mask], depth + 1)\n","        self.tree['left'] = left_subtree.tree\n","\n","        right_subtree = DecisionTreeRegressor(self.depth)\n","        right_subtree.fit(data[right_mask], target[right_mask], depth + 1)\n","        self.tree['right'] = right_subtree.tree\n","\n","\n","    def predict(self, X):\n","        # Method to handle multiple instances (rows in 2D array)\n","        if len(X.shape) != 2:\n","            raise ValueError(\"Predict method expects a 2D array of instances\")\n","        return np.array([self._predict_recursive(x, self.tree) for x in X])\n","\n","    def _predict_recursive(self, x, tree):\n","        # Internal method for recursive prediction for a single instance\n","        if 'value' in tree:\n","            return tree['value']\n","\n","        feature, theta = tree['feature'], tree['theta']\n","        if x[feature] <= theta:\n","            return self._predict_recursive(x, tree['left'])\n","        else:\n","            return self._predict_recursive(x, tree['right'])\n","\n","    # def _predict_recursive(self, x, node):\n","    #     # Internal method for recursive prediction for a single instance\n","    #     if node.value is not None:\n","    #         return node.value\n","\n","    #     if x[node.feature_index] <= node.threshold:\n","    #         return self._predict_recursive(x, node.left)\n","    #     else:\n","    #         return self._predict_recursive(x, node.right)\n","\n","\n","\n","def main():\n","  train_y , train_x = svm_read_problem(\"train.dat\")\n","  test_y, test_x = svm_read_problem(\"test.dat\")\n","  train_x = transform_X(train_x)\n","  test_x = transform_X(test_x)\n","\n","  train_x = np.array(train_x)\n","  train_y = np.array(train_y)\n","  test_x = np.array(test_x)\n","  test_y = np.array(test_y)\n","\n","  # Train model\n","  tree = DecisionTreeRegressor()\n","  tree.fit(train_x, train_y)\n","\n","  # Evaluate model\n","  predictions = tree.predict(test_x)\n","  squared_error = np.mean((y_test - predictions) ** 2)\n","  print(\"Eout: \", squared_error)\n","  # print(x)\n","  # print(y)\n","\n","\n","main()\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dfVm_D42cxqG","outputId":"a58f9e5a-38ed-4c4a-fedc-b144ee43b97f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Random_Forest( 1/2000 )\n","Random_Forest( 2/2000 )\n","Random_Forest( 3/2000 )\n","Random_Forest( 4/2000 )\n","Random_Forest( 5/2000 )\n","Random_Forest( 6/2000 )\n","Random_Forest( 7/2000 )\n","Random_Forest( 8/2000 )\n","Random_Forest( 9/2000 )\n","Random_Forest( 10/2000 )\n","Random_Forest( 11/2000 )\n","Random_Forest( 12/2000 )\n","Random_Forest( 13/2000 )\n","Random_Forest( 14/2000 )\n","Random_Forest( 15/2000 )\n","Random_Forest( 16/2000 )\n","Random_Forest( 17/2000 )\n","Random_Forest( 18/2000 )\n","Random_Forest( 19/2000 )\n","Random_Forest( 20/2000 )\n","Random_Forest( 21/2000 )\n","Random_Forest( 22/2000 )\n","Random_Forest( 23/2000 )\n","Random_Forest( 24/2000 )\n","Random_Forest( 25/2000 )\n","Random_Forest( 26/2000 )\n","Random_Forest( 27/2000 )\n","Random_Forest( 28/2000 )\n","Random_Forest( 29/2000 )\n","Random_Forest( 30/2000 )\n","Random_Forest( 31/2000 )\n","Random_Forest( 32/2000 )\n","Random_Forest( 33/2000 )\n","Random_Forest( 34/2000 )\n","Random_Forest( 35/2000 )\n","Random_Forest( 36/2000 )\n","Random_Forest( 37/2000 )\n","Random_Forest( 38/2000 )\n","Random_Forest( 39/2000 )\n","Random_Forest( 40/2000 )\n","Random_Forest( 41/2000 )\n","Random_Forest( 42/2000 )\n","Random_Forest( 43/2000 )\n","Random_Forest( 44/2000 )\n","Random_Forest( 45/2000 )\n","Random_Forest( 46/2000 )\n","Random_Forest( 47/2000 )\n","Random_Forest( 48/2000 )\n","Random_Forest( 49/2000 )\n","Random_Forest( 50/2000 )\n","Random_Forest( 51/2000 )\n","Random_Forest( 52/2000 )\n","Random_Forest( 53/2000 )\n","Random_Forest( 54/2000 )\n","Random_Forest( 55/2000 )\n","Random_Forest( 56/2000 )\n","Random_Forest( 57/2000 )\n","Random_Forest( 58/2000 )\n","Random_Forest( 59/2000 )\n","Random_Forest( 60/2000 )\n","Random_Forest( 61/2000 )\n","Random_Forest( 62/2000 )\n","Random_Forest( 63/2000 )\n","Random_Forest( 64/2000 )\n","Random_Forest( 65/2000 )\n","Random_Forest( 66/2000 )\n","Random_Forest( 67/2000 )\n","Random_Forest( 68/2000 )\n","Random_Forest( 69/2000 )\n","Random_Forest( 70/2000 )\n","Random_Forest( 71/2000 )\n","Random_Forest( 72/2000 )\n","Random_Forest( 73/2000 )\n","Random_Forest( 74/2000 )\n","Random_Forest( 75/2000 )\n","Random_Forest( 76/2000 )\n","Random_Forest( 77/2000 )\n","Random_Forest( 78/2000 )\n","Random_Forest( 79/2000 )\n","Random_Forest( 80/2000 )\n","Random_Forest( 81/2000 )\n","Random_Forest( 82/2000 )\n","Random_Forest( 83/2000 )\n","Random_Forest( 84/2000 )\n","Random_Forest( 85/2000 )\n","Random_Forest( 86/2000 )\n","Random_Forest( 87/2000 )\n","Random_Forest( 88/2000 )\n","Random_Forest( 89/2000 )\n","Random_Forest( 90/2000 )\n","Random_Forest( 91/2000 )\n","Random_Forest( 92/2000 )\n","Random_Forest( 93/2000 )\n","Random_Forest( 94/2000 )\n","Random_Forest( 95/2000 )\n","Random_Forest( 96/2000 )\n","Random_Forest( 97/2000 )\n","Random_Forest( 98/2000 )\n","Random_Forest( 99/2000 )\n","Random_Forest( 100/2000 )\n","Random_Forest( 101/2000 )\n","Random_Forest( 102/2000 )\n","Random_Forest( 103/2000 )\n","Random_Forest( 104/2000 )\n","Random_Forest( 105/2000 )\n","Random_Forest( 106/2000 )\n","Random_Forest( 107/2000 )\n","Random_Forest( 108/2000 )\n","Random_Forest( 109/2000 )\n","Random_Forest( 110/2000 )\n","Random_Forest( 111/2000 )\n","Random_Forest( 112/2000 )\n","Random_Forest( 113/2000 )\n","Random_Forest( 114/2000 )\n","Random_Forest( 115/2000 )\n","Random_Forest( 116/2000 )\n","Random_Forest( 117/2000 )\n","Random_Forest( 118/2000 )\n","Random_Forest( 119/2000 )\n","Random_Forest( 120/2000 )\n","Random_Forest( 121/2000 )\n","Random_Forest( 122/2000 )\n","Random_Forest( 123/2000 )\n","Random_Forest( 124/2000 )\n","Random_Forest( 125/2000 )\n","Random_Forest( 126/2000 )\n","Random_Forest( 127/2000 )\n","Random_Forest( 128/2000 )\n","Random_Forest( 129/2000 )\n","Random_Forest( 130/2000 )\n","Random_Forest( 131/2000 )\n","Random_Forest( 132/2000 )\n","Random_Forest( 133/2000 )\n","Random_Forest( 134/2000 )\n","Random_Forest( 135/2000 )\n","Random_Forest( 136/2000 )\n","Random_Forest( 137/2000 )\n","Random_Forest( 138/2000 )\n","Random_Forest( 139/2000 )\n","Random_Forest( 140/2000 )\n","Random_Forest( 141/2000 )\n","Random_Forest( 142/2000 )\n","Random_Forest( 143/2000 )\n","Random_Forest( 144/2000 )\n","Random_Forest( 145/2000 )\n","Random_Forest( 146/2000 )\n","Random_Forest( 147/2000 )\n","Random_Forest( 148/2000 )\n","Random_Forest( 149/2000 )\n","Random_Forest( 150/2000 )\n","Random_Forest( 151/2000 )\n","Random_Forest( 152/2000 )\n","Random_Forest( 153/2000 )\n","Random_Forest( 154/2000 )\n","Random_Forest( 155/2000 )\n","Random_Forest( 156/2000 )\n","Random_Forest( 157/2000 )\n","Random_Forest( 158/2000 )\n","Random_Forest( 159/2000 )\n","Random_Forest( 160/2000 )\n","Random_Forest( 161/2000 )\n","Random_Forest( 162/2000 )\n","Random_Forest( 163/2000 )\n","Random_Forest( 164/2000 )\n","Random_Forest( 165/2000 )\n","Random_Forest( 166/2000 )\n","Random_Forest( 167/2000 )\n","Random_Forest( 168/2000 )\n","Random_Forest( 169/2000 )\n","Random_Forest( 170/2000 )\n","Random_Forest( 171/2000 )\n","Random_Forest( 172/2000 )\n","Random_Forest( 173/2000 )\n","Random_Forest( 174/2000 )\n","Random_Forest( 175/2000 )\n","Random_Forest( 176/2000 )\n","Random_Forest( 177/2000 )\n","Random_Forest( 178/2000 )\n","Random_Forest( 179/2000 )\n","Random_Forest( 180/2000 )\n","Random_Forest( 181/2000 )\n","Random_Forest( 182/2000 )\n","Random_Forest( 183/2000 )\n","Random_Forest( 184/2000 )\n","Random_Forest( 185/2000 )\n","Random_Forest( 186/2000 )\n","Random_Forest( 187/2000 )\n","Random_Forest( 188/2000 )\n","Random_Forest( 189/2000 )\n","Random_Forest( 190/2000 )\n","Random_Forest( 191/2000 )\n","Random_Forest( 192/2000 )\n","Random_Forest( 193/2000 )\n","Random_Forest( 194/2000 )\n","Random_Forest( 195/2000 )\n","Random_Forest( 196/2000 )\n","Random_Forest( 197/2000 )\n","Random_Forest( 198/2000 )\n","Random_Forest( 199/2000 )\n","Random_Forest( 200/2000 )\n","Random_Forest( 201/2000 )\n","Random_Forest( 202/2000 )\n","Random_Forest( 203/2000 )\n","Random_Forest( 204/2000 )\n","Random_Forest( 205/2000 )\n","Random_Forest( 206/2000 )\n","Random_Forest( 207/2000 )\n","Random_Forest( 208/2000 )\n","Random_Forest( 209/2000 )\n","Random_Forest( 210/2000 )\n","Random_Forest( 211/2000 )\n","Random_Forest( 212/2000 )\n","Random_Forest( 213/2000 )\n","Random_Forest( 214/2000 )\n","Random_Forest( 215/2000 )\n","Random_Forest( 216/2000 )\n","Random_Forest( 217/2000 )\n","Random_Forest( 218/2000 )\n","Random_Forest( 219/2000 )\n","Random_Forest( 220/2000 )\n","Random_Forest( 221/2000 )\n","Random_Forest( 222/2000 )\n","Random_Forest( 223/2000 )\n","Random_Forest( 224/2000 )\n","Random_Forest( 225/2000 )\n","Random_Forest( 226/2000 )\n","Random_Forest( 227/2000 )\n","Random_Forest( 228/2000 )\n","Random_Forest( 229/2000 )\n","Random_Forest( 230/2000 )\n","Random_Forest( 231/2000 )\n","Random_Forest( 232/2000 )\n","Random_Forest( 233/2000 )\n","Random_Forest( 234/2000 )\n","Random_Forest( 235/2000 )\n","Random_Forest( 236/2000 )\n","Random_Forest( 237/2000 )\n","Random_Forest( 238/2000 )\n","Random_Forest( 239/2000 )\n","Random_Forest( 240/2000 )\n","Random_Forest( 241/2000 )\n","Random_Forest( 242/2000 )\n","Random_Forest( 243/2000 )\n","Random_Forest( 244/2000 )\n","Random_Forest( 245/2000 )\n","Random_Forest( 246/2000 )\n","Random_Forest( 247/2000 )\n","Random_Forest( 248/2000 )\n","Random_Forest( 249/2000 )\n","Random_Forest( 250/2000 )\n","Random_Forest( 251/2000 )\n","Random_Forest( 252/2000 )\n","Random_Forest( 253/2000 )\n","Random_Forest( 254/2000 )\n","Random_Forest( 255/2000 )\n","Random_Forest( 256/2000 )\n","Random_Forest( 257/2000 )\n","Random_Forest( 258/2000 )\n","Random_Forest( 259/2000 )\n","Random_Forest( 260/2000 )\n","Random_Forest( 261/2000 )\n","Random_Forest( 262/2000 )\n","Random_Forest( 263/2000 )\n","Random_Forest( 264/2000 )\n","Random_Forest( 265/2000 )\n","Random_Forest( 266/2000 )\n","Random_Forest( 267/2000 )\n","Random_Forest( 268/2000 )\n","Random_Forest( 269/2000 )\n","Random_Forest( 270/2000 )\n","Random_Forest( 271/2000 )\n","Random_Forest( 272/2000 )\n","Random_Forest( 273/2000 )\n","Random_Forest( 274/2000 )\n","Random_Forest( 275/2000 )\n","Random_Forest( 276/2000 )\n","Random_Forest( 277/2000 )\n","Random_Forest( 278/2000 )\n","Random_Forest( 279/2000 )\n","Random_Forest( 280/2000 )\n","Random_Forest( 281/2000 )\n","Random_Forest( 282/2000 )\n","Random_Forest( 283/2000 )\n","Random_Forest( 284/2000 )\n","Random_Forest( 285/2000 )\n","Random_Forest( 286/2000 )\n","Random_Forest( 287/2000 )\n","Random_Forest( 288/2000 )\n","Random_Forest( 289/2000 )\n","Random_Forest( 290/2000 )\n","Random_Forest( 291/2000 )\n","Random_Forest( 292/2000 )\n","Random_Forest( 293/2000 )\n","Random_Forest( 294/2000 )\n","Random_Forest( 295/2000 )\n","Random_Forest( 296/2000 )\n","Random_Forest( 297/2000 )\n","Random_Forest( 298/2000 )\n","Random_Forest( 299/2000 )\n","Random_Forest( 300/2000 )\n","Random_Forest( 301/2000 )\n","Random_Forest( 302/2000 )\n","Random_Forest( 303/2000 )\n","Random_Forest( 304/2000 )\n","Random_Forest( 305/2000 )\n","Random_Forest( 306/2000 )\n","Random_Forest( 307/2000 )\n","Random_Forest( 308/2000 )\n","Random_Forest( 309/2000 )\n","Random_Forest( 310/2000 )\n","Random_Forest( 311/2000 )\n","Random_Forest( 312/2000 )\n","Random_Forest( 313/2000 )\n","Random_Forest( 314/2000 )\n","Random_Forest( 315/2000 )\n","Random_Forest( 316/2000 )\n","Random_Forest( 317/2000 )\n","Random_Forest( 318/2000 )\n","Random_Forest( 319/2000 )\n","Random_Forest( 320/2000 )\n","Random_Forest( 321/2000 )\n","Random_Forest( 322/2000 )\n","Random_Forest( 323/2000 )\n","Random_Forest( 324/2000 )\n","Random_Forest( 325/2000 )\n","Random_Forest( 326/2000 )\n","Random_Forest( 327/2000 )\n","Random_Forest( 328/2000 )\n","Random_Forest( 329/2000 )\n","Random_Forest( 330/2000 )\n","Random_Forest( 331/2000 )\n","Random_Forest( 332/2000 )\n","Random_Forest( 333/2000 )\n","Random_Forest( 334/2000 )\n","Random_Forest( 335/2000 )\n","Random_Forest( 336/2000 )\n","Random_Forest( 337/2000 )\n","Random_Forest( 338/2000 )\n","Random_Forest( 339/2000 )\n","Random_Forest( 340/2000 )\n","Random_Forest( 341/2000 )\n","Random_Forest( 342/2000 )\n","Random_Forest( 343/2000 )\n","Random_Forest( 344/2000 )\n","Random_Forest( 345/2000 )\n","Random_Forest( 346/2000 )\n","Random_Forest( 347/2000 )\n","Random_Forest( 348/2000 )\n","Random_Forest( 349/2000 )\n","Random_Forest( 350/2000 )\n","Random_Forest( 351/2000 )\n","Random_Forest( 352/2000 )\n","Random_Forest( 353/2000 )\n","Random_Forest( 354/2000 )\n","Random_Forest( 355/2000 )\n","Random_Forest( 356/2000 )\n","Random_Forest( 357/2000 )\n","Random_Forest( 358/2000 )\n","Random_Forest( 359/2000 )\n","Random_Forest( 360/2000 )\n","Random_Forest( 361/2000 )\n","Random_Forest( 362/2000 )\n","Random_Forest( 363/2000 )\n","Random_Forest( 364/2000 )\n","Random_Forest( 365/2000 )\n","Random_Forest( 366/2000 )\n","Random_Forest( 367/2000 )\n","Random_Forest( 368/2000 )\n","Random_Forest( 369/2000 )\n","Random_Forest( 370/2000 )\n","Random_Forest( 371/2000 )\n","Random_Forest( 372/2000 )\n","Random_Forest( 373/2000 )\n","Random_Forest( 374/2000 )\n","Random_Forest( 375/2000 )\n","Random_Forest( 376/2000 )\n","Random_Forest( 377/2000 )\n","Random_Forest( 378/2000 )\n","Random_Forest( 379/2000 )\n","Random_Forest( 380/2000 )\n","Random_Forest( 381/2000 )\n","Random_Forest( 382/2000 )\n","Random_Forest( 383/2000 )\n","Random_Forest( 384/2000 )\n","Random_Forest( 385/2000 )\n","Random_Forest( 386/2000 )\n","Random_Forest( 387/2000 )\n","Random_Forest( 388/2000 )\n","Random_Forest( 389/2000 )\n","Random_Forest( 390/2000 )\n","Random_Forest( 391/2000 )\n","Random_Forest( 392/2000 )\n","Random_Forest( 393/2000 )\n","Random_Forest( 394/2000 )\n","Random_Forest( 395/2000 )\n","Random_Forest( 396/2000 )\n","Random_Forest( 397/2000 )\n","Random_Forest( 398/2000 )\n","Random_Forest( 399/2000 )\n","Random_Forest( 400/2000 )\n","Random_Forest( 401/2000 )\n","Random_Forest( 402/2000 )\n","Random_Forest( 403/2000 )\n","Random_Forest( 404/2000 )\n","Random_Forest( 405/2000 )\n","Random_Forest( 406/2000 )\n","Random_Forest( 407/2000 )\n","Random_Forest( 408/2000 )\n","Random_Forest( 409/2000 )\n","Random_Forest( 410/2000 )\n","Random_Forest( 411/2000 )\n","Random_Forest( 412/2000 )\n","Random_Forest( 413/2000 )\n","Random_Forest( 414/2000 )\n","Random_Forest( 415/2000 )\n","Random_Forest( 416/2000 )\n","Random_Forest( 417/2000 )\n","Random_Forest( 418/2000 )\n","Random_Forest( 419/2000 )\n","Random_Forest( 420/2000 )\n","Random_Forest( 421/2000 )\n","Random_Forest( 422/2000 )\n","Random_Forest( 423/2000 )\n","Random_Forest( 424/2000 )\n","Random_Forest( 425/2000 )\n","Random_Forest( 426/2000 )\n","Random_Forest( 427/2000 )\n","Random_Forest( 428/2000 )\n","Random_Forest( 429/2000 )\n","Random_Forest( 430/2000 )\n","Random_Forest( 431/2000 )\n","Random_Forest( 432/2000 )\n","Random_Forest( 433/2000 )\n","Random_Forest( 434/2000 )\n","Random_Forest( 435/2000 )\n","Random_Forest( 436/2000 )\n","Random_Forest( 437/2000 )\n","Random_Forest( 438/2000 )\n","Random_Forest( 439/2000 )\n","Random_Forest( 440/2000 )\n","Random_Forest( 441/2000 )\n","Random_Forest( 442/2000 )\n","Random_Forest( 443/2000 )\n","Random_Forest( 444/2000 )\n","Random_Forest( 445/2000 )\n","Random_Forest( 446/2000 )\n","Random_Forest( 447/2000 )\n","Random_Forest( 448/2000 )\n","Random_Forest( 449/2000 )\n","Random_Forest( 450/2000 )\n","Random_Forest( 451/2000 )\n","Random_Forest( 452/2000 )\n","Random_Forest( 453/2000 )\n","Random_Forest( 454/2000 )\n","Random_Forest( 455/2000 )\n","Random_Forest( 456/2000 )\n","Random_Forest( 457/2000 )\n","Random_Forest( 458/2000 )\n","Random_Forest( 459/2000 )\n","Random_Forest( 460/2000 )\n","Random_Forest( 461/2000 )\n","Random_Forest( 462/2000 )\n","Random_Forest( 463/2000 )\n","Random_Forest( 464/2000 )\n","Random_Forest( 465/2000 )\n","Random_Forest( 466/2000 )\n","Random_Forest( 467/2000 )\n","Random_Forest( 468/2000 )\n","Random_Forest( 469/2000 )\n","Random_Forest( 470/2000 )\n","Random_Forest( 471/2000 )\n","Random_Forest( 472/2000 )\n","Random_Forest( 473/2000 )\n","Random_Forest( 474/2000 )\n","Random_Forest( 475/2000 )\n","Random_Forest( 476/2000 )\n","Random_Forest( 477/2000 )\n","Random_Forest( 478/2000 )\n","Random_Forest( 479/2000 )\n","Random_Forest( 480/2000 )\n","Random_Forest( 481/2000 )\n","Random_Forest( 482/2000 )\n","Random_Forest( 483/2000 )\n","Random_Forest( 484/2000 )\n","Random_Forest( 485/2000 )\n","Random_Forest( 486/2000 )\n","Random_Forest( 487/2000 )\n","Random_Forest( 488/2000 )\n","Random_Forest( 489/2000 )\n","Random_Forest( 490/2000 )\n","Random_Forest( 491/2000 )\n","Random_Forest( 492/2000 )\n","Random_Forest( 493/2000 )\n","Random_Forest( 494/2000 )\n","Random_Forest( 495/2000 )\n","Random_Forest( 496/2000 )\n","Random_Forest( 497/2000 )\n","Random_Forest( 498/2000 )\n","Random_Forest( 499/2000 )\n","Random_Forest( 500/2000 )\n","Random_Forest( 501/2000 )\n","Random_Forest( 502/2000 )\n","Random_Forest( 503/2000 )\n","Random_Forest( 504/2000 )\n","Random_Forest( 505/2000 )\n","Random_Forest( 506/2000 )\n","Random_Forest( 507/2000 )\n","Random_Forest( 508/2000 )\n","Random_Forest( 509/2000 )\n","Random_Forest( 510/2000 )\n","Random_Forest( 511/2000 )\n","Random_Forest( 512/2000 )\n","Random_Forest( 513/2000 )\n","Random_Forest( 514/2000 )\n","Random_Forest( 515/2000 )\n","Random_Forest( 516/2000 )\n","Random_Forest( 517/2000 )\n","Random_Forest( 518/2000 )\n","Random_Forest( 519/2000 )\n","Random_Forest( 520/2000 )\n","Random_Forest( 521/2000 )\n","Random_Forest( 522/2000 )\n","Random_Forest( 523/2000 )\n","Random_Forest( 524/2000 )\n","Random_Forest( 525/2000 )\n","Random_Forest( 526/2000 )\n","Random_Forest( 527/2000 )\n","Random_Forest( 528/2000 )\n","Random_Forest( 529/2000 )\n","Random_Forest( 530/2000 )\n","Random_Forest( 531/2000 )\n","Random_Forest( 532/2000 )\n","Random_Forest( 533/2000 )\n","Random_Forest( 534/2000 )\n","Random_Forest( 535/2000 )\n","Random_Forest( 536/2000 )\n","Random_Forest( 537/2000 )\n","Random_Forest( 538/2000 )\n","Random_Forest( 539/2000 )\n","Random_Forest( 540/2000 )\n","Random_Forest( 541/2000 )\n","Random_Forest( 542/2000 )\n","Random_Forest( 543/2000 )\n","Random_Forest( 544/2000 )\n","Random_Forest( 545/2000 )\n","Random_Forest( 546/2000 )\n","Random_Forest( 547/2000 )\n","Random_Forest( 548/2000 )\n","Random_Forest( 549/2000 )\n","Random_Forest( 550/2000 )\n","Random_Forest( 551/2000 )\n","Random_Forest( 552/2000 )\n","Random_Forest( 553/2000 )\n","Random_Forest( 554/2000 )\n","Random_Forest( 555/2000 )\n","Random_Forest( 556/2000 )\n","Random_Forest( 557/2000 )\n","Random_Forest( 558/2000 )\n","Random_Forest( 559/2000 )\n","Random_Forest( 560/2000 )\n","Random_Forest( 561/2000 )\n","Random_Forest( 562/2000 )\n","Random_Forest( 563/2000 )\n","Random_Forest( 564/2000 )\n","Random_Forest( 565/2000 )\n","Random_Forest( 566/2000 )\n","Random_Forest( 567/2000 )\n","Random_Forest( 568/2000 )\n","Random_Forest( 569/2000 )\n","Random_Forest( 570/2000 )\n","Random_Forest( 571/2000 )\n","Random_Forest( 572/2000 )\n","Random_Forest( 573/2000 )\n","Random_Forest( 574/2000 )\n","Random_Forest( 575/2000 )\n","Random_Forest( 576/2000 )\n","Random_Forest( 577/2000 )\n","Random_Forest( 578/2000 )\n","Random_Forest( 579/2000 )\n","Random_Forest( 580/2000 )\n","Random_Forest( 581/2000 )\n","Random_Forest( 582/2000 )\n","Random_Forest( 583/2000 )\n","Random_Forest( 584/2000 )\n","Random_Forest( 585/2000 )\n","Random_Forest( 586/2000 )\n","Random_Forest( 587/2000 )\n","Random_Forest( 588/2000 )\n","Random_Forest( 589/2000 )\n","Random_Forest( 590/2000 )\n","Random_Forest( 591/2000 )\n","Random_Forest( 592/2000 )\n","Random_Forest( 593/2000 )\n","Random_Forest( 594/2000 )\n","Random_Forest( 595/2000 )\n","Random_Forest( 596/2000 )\n","Random_Forest( 597/2000 )\n","Random_Forest( 598/2000 )\n","Random_Forest( 599/2000 )\n","Random_Forest( 600/2000 )\n","Random_Forest( 601/2000 )\n","Random_Forest( 602/2000 )\n","Random_Forest( 603/2000 )\n","Random_Forest( 604/2000 )\n","Random_Forest( 605/2000 )\n","Random_Forest( 606/2000 )\n","Random_Forest( 607/2000 )\n","Random_Forest( 608/2000 )\n","Random_Forest( 609/2000 )\n","Random_Forest( 610/2000 )\n","Random_Forest( 611/2000 )\n","Random_Forest( 612/2000 )\n","Random_Forest( 613/2000 )\n","Random_Forest( 614/2000 )\n","Random_Forest( 615/2000 )\n","Random_Forest( 616/2000 )\n","Random_Forest( 617/2000 )\n","Random_Forest( 618/2000 )\n","Random_Forest( 619/2000 )\n","Random_Forest( 620/2000 )\n","Random_Forest( 621/2000 )\n","Random_Forest( 622/2000 )\n","Random_Forest( 623/2000 )\n","Random_Forest( 624/2000 )\n","Random_Forest( 625/2000 )\n","Random_Forest( 626/2000 )\n","Random_Forest( 627/2000 )\n","Random_Forest( 628/2000 )\n","Random_Forest( 629/2000 )\n","Random_Forest( 630/2000 )\n","Random_Forest( 631/2000 )\n","Random_Forest( 632/2000 )\n","Random_Forest( 633/2000 )\n","Random_Forest( 634/2000 )\n","Random_Forest( 635/2000 )\n","Random_Forest( 636/2000 )\n","Random_Forest( 637/2000 )\n","Random_Forest( 638/2000 )\n","Random_Forest( 639/2000 )\n","Random_Forest( 640/2000 )\n","Random_Forest( 641/2000 )\n","Random_Forest( 642/2000 )\n","Random_Forest( 643/2000 )\n","Random_Forest( 644/2000 )\n","Random_Forest( 645/2000 )\n","Random_Forest( 646/2000 )\n","Random_Forest( 647/2000 )\n","Random_Forest( 648/2000 )\n","Random_Forest( 649/2000 )\n","Random_Forest( 650/2000 )\n","Random_Forest( 651/2000 )\n","Random_Forest( 652/2000 )\n","Random_Forest( 653/2000 )\n","Random_Forest( 654/2000 )\n","Random_Forest( 655/2000 )\n","Random_Forest( 656/2000 )\n","Random_Forest( 657/2000 )\n","Random_Forest( 658/2000 )\n","Random_Forest( 659/2000 )\n","Random_Forest( 660/2000 )\n","Random_Forest( 661/2000 )\n","Random_Forest( 662/2000 )\n","Random_Forest( 663/2000 )\n","Random_Forest( 664/2000 )\n","Random_Forest( 665/2000 )\n","Random_Forest( 666/2000 )\n","Random_Forest( 667/2000 )\n","Random_Forest( 668/2000 )\n","Random_Forest( 669/2000 )\n","Random_Forest( 670/2000 )\n","Random_Forest( 671/2000 )\n","Random_Forest( 672/2000 )\n","Random_Forest( 673/2000 )\n","Random_Forest( 674/2000 )\n","Random_Forest( 675/2000 )\n","Random_Forest( 676/2000 )\n","Random_Forest( 677/2000 )\n","Random_Forest( 678/2000 )\n","Random_Forest( 679/2000 )\n","Random_Forest( 680/2000 )\n","Random_Forest( 681/2000 )\n","Random_Forest( 682/2000 )\n","Random_Forest( 683/2000 )\n","Random_Forest( 684/2000 )\n","Random_Forest( 685/2000 )\n","Random_Forest( 686/2000 )\n","Random_Forest( 687/2000 )\n","Random_Forest( 688/2000 )\n","Random_Forest( 689/2000 )\n","Random_Forest( 690/2000 )\n","Random_Forest( 691/2000 )\n","Random_Forest( 692/2000 )\n","Random_Forest( 693/2000 )\n","Random_Forest( 694/2000 )\n","Random_Forest( 695/2000 )\n","Random_Forest( 696/2000 )\n","Random_Forest( 697/2000 )\n","Random_Forest( 698/2000 )\n","Random_Forest( 699/2000 )\n","Random_Forest( 700/2000 )\n","Random_Forest( 701/2000 )\n","Random_Forest( 702/2000 )\n","Random_Forest( 703/2000 )\n","Random_Forest( 704/2000 )\n","Random_Forest( 705/2000 )\n","Random_Forest( 706/2000 )\n","Random_Forest( 707/2000 )\n","Random_Forest( 708/2000 )\n","Random_Forest( 709/2000 )\n","Random_Forest( 710/2000 )\n","Random_Forest( 711/2000 )\n","Random_Forest( 712/2000 )\n","Random_Forest( 713/2000 )\n","Random_Forest( 714/2000 )\n","Random_Forest( 715/2000 )\n","Random_Forest( 716/2000 )\n","Random_Forest( 717/2000 )\n","Random_Forest( 718/2000 )\n","Random_Forest( 719/2000 )\n","Random_Forest( 720/2000 )\n","Random_Forest( 721/2000 )\n","Random_Forest( 722/2000 )\n","Random_Forest( 723/2000 )\n","Random_Forest( 724/2000 )\n","Random_Forest( 725/2000 )\n","Random_Forest( 726/2000 )\n","Random_Forest( 727/2000 )\n","Random_Forest( 728/2000 )\n","Random_Forest( 729/2000 )\n","Random_Forest( 730/2000 )\n","Random_Forest( 731/2000 )\n","Random_Forest( 732/2000 )\n","Random_Forest( 733/2000 )\n","Random_Forest( 734/2000 )\n","Random_Forest( 735/2000 )\n","Random_Forest( 736/2000 )\n","Random_Forest( 737/2000 )\n","Random_Forest( 738/2000 )\n","Random_Forest( 739/2000 )\n","Random_Forest( 740/2000 )\n","Random_Forest( 741/2000 )\n","Random_Forest( 742/2000 )\n","Random_Forest( 743/2000 )\n","Random_Forest( 744/2000 )\n","Random_Forest( 745/2000 )\n","Random_Forest( 746/2000 )\n","Random_Forest( 747/2000 )\n","Random_Forest( 748/2000 )\n","Random_Forest( 749/2000 )\n","Random_Forest( 750/2000 )\n","Random_Forest( 751/2000 )\n","Random_Forest( 752/2000 )\n","Random_Forest( 753/2000 )\n","Random_Forest( 754/2000 )\n","Random_Forest( 755/2000 )\n","Random_Forest( 756/2000 )\n","Random_Forest( 757/2000 )\n","Random_Forest( 758/2000 )\n","Random_Forest( 759/2000 )\n","Random_Forest( 760/2000 )\n","Random_Forest( 761/2000 )\n","Random_Forest( 762/2000 )\n","Random_Forest( 763/2000 )\n","Random_Forest( 764/2000 )\n","Random_Forest( 765/2000 )\n","Random_Forest( 766/2000 )\n","Random_Forest( 767/2000 )\n","Random_Forest( 768/2000 )\n","Random_Forest( 769/2000 )\n","Random_Forest( 770/2000 )\n","Random_Forest( 771/2000 )\n","Random_Forest( 772/2000 )\n","Random_Forest( 773/2000 )\n","Random_Forest( 774/2000 )\n","Random_Forest( 775/2000 )\n","Random_Forest( 776/2000 )\n","Random_Forest( 777/2000 )\n","Random_Forest( 778/2000 )\n","Random_Forest( 779/2000 )\n","Random_Forest( 780/2000 )\n","Random_Forest( 781/2000 )\n","Random_Forest( 782/2000 )\n","Random_Forest( 783/2000 )\n","Random_Forest( 784/2000 )\n","Random_Forest( 785/2000 )\n","Random_Forest( 786/2000 )\n","Random_Forest( 787/2000 )\n","Random_Forest( 788/2000 )\n","Random_Forest( 789/2000 )\n","Random_Forest( 790/2000 )\n","Random_Forest( 791/2000 )\n","Random_Forest( 792/2000 )\n","Random_Forest( 793/2000 )\n","Random_Forest( 794/2000 )\n","Random_Forest( 795/2000 )\n","Random_Forest( 796/2000 )\n","Random_Forest( 797/2000 )\n","Random_Forest( 798/2000 )\n","Random_Forest( 799/2000 )\n","Random_Forest( 800/2000 )\n","Random_Forest( 801/2000 )\n","Random_Forest( 802/2000 )\n","Random_Forest( 803/2000 )\n","Random_Forest( 804/2000 )\n","Random_Forest( 805/2000 )\n","Random_Forest( 806/2000 )\n","Random_Forest( 807/2000 )\n","Random_Forest( 808/2000 )\n","Random_Forest( 809/2000 )\n","Random_Forest( 810/2000 )\n","Random_Forest( 811/2000 )\n","Random_Forest( 812/2000 )\n","Random_Forest( 813/2000 )\n","Random_Forest( 814/2000 )\n","Random_Forest( 815/2000 )\n","Random_Forest( 816/2000 )\n","Random_Forest( 817/2000 )\n","Random_Forest( 818/2000 )\n","Random_Forest( 819/2000 )\n","Random_Forest( 820/2000 )\n","Random_Forest( 821/2000 )\n","Random_Forest( 822/2000 )\n","Random_Forest( 823/2000 )\n","Random_Forest( 824/2000 )\n","Random_Forest( 825/2000 )\n","Random_Forest( 826/2000 )\n","Random_Forest( 827/2000 )\n","Random_Forest( 828/2000 )\n","Random_Forest( 829/2000 )\n","Random_Forest( 830/2000 )\n","Random_Forest( 831/2000 )\n","Random_Forest( 832/2000 )\n","Random_Forest( 833/2000 )\n","Random_Forest( 834/2000 )\n","Random_Forest( 835/2000 )\n","Random_Forest( 836/2000 )\n","Random_Forest( 837/2000 )\n","Random_Forest( 838/2000 )\n","Random_Forest( 839/2000 )\n","Random_Forest( 840/2000 )\n","Random_Forest( 841/2000 )\n","Random_Forest( 842/2000 )\n","Random_Forest( 843/2000 )\n","Random_Forest( 844/2000 )\n","Random_Forest( 845/2000 )\n","Random_Forest( 846/2000 )\n","Random_Forest( 847/2000 )\n","Random_Forest( 848/2000 )\n","Random_Forest( 849/2000 )\n","Random_Forest( 850/2000 )\n","Random_Forest( 851/2000 )\n","Random_Forest( 852/2000 )\n","Random_Forest( 853/2000 )\n","Random_Forest( 854/2000 )\n","Random_Forest( 855/2000 )\n","Random_Forest( 856/2000 )\n","Random_Forest( 857/2000 )\n","Random_Forest( 858/2000 )\n","Random_Forest( 859/2000 )\n","Random_Forest( 860/2000 )\n","Random_Forest( 861/2000 )\n","Random_Forest( 862/2000 )\n","Random_Forest( 863/2000 )\n","Random_Forest( 864/2000 )\n","Random_Forest( 865/2000 )\n","Random_Forest( 866/2000 )\n","Random_Forest( 867/2000 )\n","Random_Forest( 868/2000 )\n","Random_Forest( 869/2000 )\n","Random_Forest( 870/2000 )\n","Random_Forest( 871/2000 )\n","Random_Forest( 872/2000 )\n","Random_Forest( 873/2000 )\n","Random_Forest( 874/2000 )\n","Random_Forest( 875/2000 )\n","Random_Forest( 876/2000 )\n","Random_Forest( 877/2000 )\n","Random_Forest( 878/2000 )\n","Random_Forest( 879/2000 )\n","Random_Forest( 880/2000 )\n","Random_Forest( 881/2000 )\n","Random_Forest( 882/2000 )\n","Random_Forest( 883/2000 )\n","Random_Forest( 884/2000 )\n","Random_Forest( 885/2000 )\n","Random_Forest( 886/2000 )\n","Random_Forest( 887/2000 )\n","Random_Forest( 888/2000 )\n","Random_Forest( 889/2000 )\n","Random_Forest( 890/2000 )\n","Random_Forest( 891/2000 )\n","Random_Forest( 892/2000 )\n","Random_Forest( 893/2000 )\n","Random_Forest( 894/2000 )\n","Random_Forest( 895/2000 )\n","Random_Forest( 896/2000 )\n","Random_Forest( 897/2000 )\n","Random_Forest( 898/2000 )\n","Random_Forest( 899/2000 )\n","Random_Forest( 900/2000 )\n","Random_Forest( 901/2000 )\n","Random_Forest( 902/2000 )\n","Random_Forest( 903/2000 )\n","Random_Forest( 904/2000 )\n","Random_Forest( 905/2000 )\n","Random_Forest( 906/2000 )\n","Random_Forest( 907/2000 )\n","Random_Forest( 908/2000 )\n","Random_Forest( 909/2000 )\n","Random_Forest( 910/2000 )\n","Random_Forest( 911/2000 )\n","Random_Forest( 912/2000 )\n","Random_Forest( 913/2000 )\n","Random_Forest( 914/2000 )\n","Random_Forest( 915/2000 )\n","Random_Forest( 916/2000 )\n","Random_Forest( 917/2000 )\n","Random_Forest( 918/2000 )\n","Random_Forest( 919/2000 )\n","Random_Forest( 920/2000 )\n","Random_Forest( 921/2000 )\n","Random_Forest( 922/2000 )\n","Random_Forest( 923/2000 )\n","Random_Forest( 924/2000 )\n","Random_Forest( 925/2000 )\n","Random_Forest( 926/2000 )\n","Random_Forest( 927/2000 )\n","Random_Forest( 928/2000 )\n","Random_Forest( 929/2000 )\n","Random_Forest( 930/2000 )\n","Random_Forest( 931/2000 )\n","Random_Forest( 932/2000 )\n","Random_Forest( 933/2000 )\n","Random_Forest( 934/2000 )\n","Random_Forest( 935/2000 )\n","Random_Forest( 936/2000 )\n","Random_Forest( 937/2000 )\n","Random_Forest( 938/2000 )\n","Random_Forest( 939/2000 )\n","Random_Forest( 940/2000 )\n","Random_Forest( 941/2000 )\n","Random_Forest( 942/2000 )\n","Random_Forest( 943/2000 )\n","Random_Forest( 944/2000 )\n","Random_Forest( 945/2000 )\n","Random_Forest( 946/2000 )\n","Random_Forest( 947/2000 )\n","Random_Forest( 948/2000 )\n","Random_Forest( 949/2000 )\n","Random_Forest( 950/2000 )\n","Random_Forest( 951/2000 )\n","Random_Forest( 952/2000 )\n","Random_Forest( 953/2000 )\n","Random_Forest( 954/2000 )\n","Random_Forest( 955/2000 )\n","Random_Forest( 956/2000 )\n","Random_Forest( 957/2000 )\n","Random_Forest( 958/2000 )\n","Random_Forest( 959/2000 )\n","Random_Forest( 960/2000 )\n","Random_Forest( 961/2000 )\n","Random_Forest( 962/2000 )\n","Random_Forest( 963/2000 )\n","Random_Forest( 964/2000 )\n","Random_Forest( 965/2000 )\n","Random_Forest( 966/2000 )\n","Random_Forest( 967/2000 )\n","Random_Forest( 968/2000 )\n","Random_Forest( 969/2000 )\n","Random_Forest( 970/2000 )\n","Random_Forest( 971/2000 )\n","Random_Forest( 972/2000 )\n","Random_Forest( 973/2000 )\n","Random_Forest( 974/2000 )\n","Random_Forest( 975/2000 )\n","Random_Forest( 976/2000 )\n","Random_Forest( 977/2000 )\n","Random_Forest( 978/2000 )\n","Random_Forest( 979/2000 )\n","Random_Forest( 980/2000 )\n","Random_Forest( 981/2000 )\n","Random_Forest( 982/2000 )\n","Random_Forest( 983/2000 )\n","Random_Forest( 984/2000 )\n","Random_Forest( 985/2000 )\n","Random_Forest( 986/2000 )\n","Random_Forest( 987/2000 )\n","Random_Forest( 988/2000 )\n","Random_Forest( 989/2000 )\n","Random_Forest( 990/2000 )\n","Random_Forest( 991/2000 )\n","Random_Forest( 992/2000 )\n","Random_Forest( 993/2000 )\n","Random_Forest( 994/2000 )\n","Random_Forest( 995/2000 )\n","Random_Forest( 996/2000 )\n","Random_Forest( 997/2000 )\n","Random_Forest( 998/2000 )\n","Random_Forest( 999/2000 )\n","Random_Forest( 1000/2000 )\n","Random_Forest( 1001/2000 )\n","Random_Forest( 1002/2000 )\n","Random_Forest( 1003/2000 )\n","Random_Forest( 1004/2000 )\n","Random_Forest( 1005/2000 )\n","Random_Forest( 1006/2000 )\n","Random_Forest( 1007/2000 )\n","Random_Forest( 1008/2000 )\n","Random_Forest( 1009/2000 )\n","Random_Forest( 1010/2000 )\n","Random_Forest( 1011/2000 )\n","Random_Forest( 1012/2000 )\n","Random_Forest( 1013/2000 )\n","Random_Forest( 1014/2000 )\n","Random_Forest( 1015/2000 )\n","Random_Forest( 1016/2000 )\n","Random_Forest( 1017/2000 )\n","Random_Forest( 1018/2000 )\n","Random_Forest( 1019/2000 )\n","Random_Forest( 1020/2000 )\n","Random_Forest( 1021/2000 )\n","Random_Forest( 1022/2000 )\n","Random_Forest( 1023/2000 )\n","Random_Forest( 1024/2000 )\n","Random_Forest( 1025/2000 )\n","Random_Forest( 1026/2000 )\n","Random_Forest( 1027/2000 )\n","Random_Forest( 1028/2000 )\n","Random_Forest( 1029/2000 )\n","Random_Forest( 1030/2000 )\n","Random_Forest( 1031/2000 )\n","Random_Forest( 1032/2000 )\n","Random_Forest( 1033/2000 )\n","Random_Forest( 1034/2000 )\n","Random_Forest( 1035/2000 )\n","Random_Forest( 1036/2000 )\n","Random_Forest( 1037/2000 )\n","Random_Forest( 1038/2000 )\n","Random_Forest( 1039/2000 )\n","Random_Forest( 1040/2000 )\n","Random_Forest( 1041/2000 )\n","Random_Forest( 1042/2000 )\n","Random_Forest( 1043/2000 )\n","Random_Forest( 1044/2000 )\n","Random_Forest( 1045/2000 )\n","Random_Forest( 1046/2000 )\n","Random_Forest( 1047/2000 )\n","Random_Forest( 1048/2000 )\n","Random_Forest( 1049/2000 )\n","Random_Forest( 1050/2000 )\n","Random_Forest( 1051/2000 )\n","Random_Forest( 1052/2000 )\n","Random_Forest( 1053/2000 )\n","Random_Forest( 1054/2000 )\n","Random_Forest( 1055/2000 )\n","Random_Forest( 1056/2000 )\n","Random_Forest( 1057/2000 )\n","Random_Forest( 1058/2000 )\n","Random_Forest( 1059/2000 )\n","Random_Forest( 1060/2000 )\n","Random_Forest( 1061/2000 )\n","Random_Forest( 1062/2000 )\n","Random_Forest( 1063/2000 )\n","Random_Forest( 1064/2000 )\n","Random_Forest( 1065/2000 )\n","Random_Forest( 1066/2000 )\n","Random_Forest( 1067/2000 )\n","Random_Forest( 1068/2000 )\n","Random_Forest( 1069/2000 )\n","Random_Forest( 1070/2000 )\n","Random_Forest( 1071/2000 )\n","Random_Forest( 1072/2000 )\n","Random_Forest( 1073/2000 )\n","Random_Forest( 1074/2000 )\n","Random_Forest( 1075/2000 )\n","Random_Forest( 1076/2000 )\n","Random_Forest( 1077/2000 )\n","Random_Forest( 1078/2000 )\n","Random_Forest( 1079/2000 )\n","Random_Forest( 1080/2000 )\n","Random_Forest( 1081/2000 )\n","Random_Forest( 1082/2000 )\n","Random_Forest( 1083/2000 )\n","Random_Forest( 1084/2000 )\n","Random_Forest( 1085/2000 )\n","Random_Forest( 1086/2000 )\n","Random_Forest( 1087/2000 )\n","Random_Forest( 1088/2000 )\n","Random_Forest( 1089/2000 )\n","Random_Forest( 1090/2000 )\n","Random_Forest( 1091/2000 )\n","Random_Forest( 1092/2000 )\n","Random_Forest( 1093/2000 )\n","Random_Forest( 1094/2000 )\n","Random_Forest( 1095/2000 )\n","Random_Forest( 1096/2000 )\n","Random_Forest( 1097/2000 )\n","Random_Forest( 1098/2000 )\n","Random_Forest( 1099/2000 )\n","Random_Forest( 1100/2000 )\n","Random_Forest( 1101/2000 )\n","Random_Forest( 1102/2000 )\n","Random_Forest( 1103/2000 )\n","Random_Forest( 1104/2000 )\n","Random_Forest( 1105/2000 )\n","Random_Forest( 1106/2000 )\n","Random_Forest( 1107/2000 )\n","Random_Forest( 1108/2000 )\n","Random_Forest( 1109/2000 )\n","Random_Forest( 1110/2000 )\n","Random_Forest( 1111/2000 )\n","Random_Forest( 1112/2000 )\n","Random_Forest( 1113/2000 )\n","Random_Forest( 1114/2000 )\n","Random_Forest( 1115/2000 )\n","Random_Forest( 1116/2000 )\n","Random_Forest( 1117/2000 )\n","Random_Forest( 1118/2000 )\n","Random_Forest( 1119/2000 )\n","Random_Forest( 1120/2000 )\n","Random_Forest( 1121/2000 )\n","Random_Forest( 1122/2000 )\n","Random_Forest( 1123/2000 )\n","Random_Forest( 1124/2000 )\n","Random_Forest( 1125/2000 )\n","Random_Forest( 1126/2000 )\n","Random_Forest( 1127/2000 )\n","Random_Forest( 1128/2000 )\n","Random_Forest( 1129/2000 )\n","Random_Forest( 1130/2000 )\n","Random_Forest( 1131/2000 )\n","Random_Forest( 1132/2000 )\n","Random_Forest( 1133/2000 )\n","Random_Forest( 1134/2000 )\n","Random_Forest( 1135/2000 )\n","Random_Forest( 1136/2000 )\n","Random_Forest( 1137/2000 )\n","Random_Forest( 1138/2000 )\n","Random_Forest( 1139/2000 )\n","Random_Forest( 1140/2000 )\n","Random_Forest( 1141/2000 )\n","Random_Forest( 1142/2000 )\n"]}],"source":["# p10\n","\n","from libsvm.svmutil import *\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","\n","def transform_X(x):\n","  X = np.array([0,0,0,0,0,0,0,0])\n","  # X = np.empty(shape = (len(x), 8))\n","  for i in range(len(x)):\n","    dic = x[i]\n","    rows = [value for key,value in dic.items()]\n","    X = np.vstack([X, rows])\n","\n","  X = np.delete(X,0,0)\n","  return X\n","\n","\n","def compute_squared_error(y_true):\n","    return np.mean((y_true - np.mean(y_true)) ** 2)\n","\n","\n","def find_best_split(data, target):\n","    # Ensure data is a 2D numpy array and target is a 1D numpy array\n","    if len(data.shape) != 2 or len(target.shape) != 1:\n","        raise ValueError(\"Data must be a 2D array and target must be a 1D array\")\n","\n","    n_features = data.shape[1]\n","    best_feature, best_theta, min_error = None, None, float('inf')\n","\n","    for feature in range(n_features):\n","        sorted_values = np.unique(np.sort(data[:, feature]))\n","        thetas = (sorted_values[:-1] + sorted_values[1:]) / 2\n","\n","        for theta in thetas:\n","            left_mask = data[:, feature] <= theta\n","            right_mask = ~left_mask\n","\n","            left_target = target[left_mask]\n","            right_target = target[right_mask]\n","\n","            left_error = compute_squared_error(left_target) if left_target.size > 0 else 0\n","            right_error = compute_squared_error(right_target) if right_target.size > 0 else 0\n","\n","            total_error = left_error * left_mask.sum() + right_error * right_mask.sum()\n","\n","            if total_error < min_error:\n","                best_feature, best_theta, min_error = feature, theta, total_error\n","\n","    return best_feature, best_theta\n","\n","\n","class Node:\n","    def __init__(self, feature_index=None, threshold=None, left=None, right=None, value=None):\n","        self.feature_index = feature_index\n","        self.threshold = threshold\n","        self.left = left\n","        self.right = right\n","        self.value = value\n","\n","class DecisionTreeRegressor:\n","    def __init__(self, depth = None):\n","        self.depth = depth\n","        self.tree = {}\n","\n","    def fit(self, data, target, depth=0):\n","      # self.root = self.fit(data, target)\n","        if depth == self.depth or data.shape[0] < 2:\n","            self.tree['value'] = target.mean()\n","            return\n","\n","        best_feature, best_theta = find_best_split(data, target)\n","        if best_feature is None:\n","            self.tree['value'] = target.mean()\n","            return\n","\n","        self.tree = {'feature': best_feature, 'theta': best_theta, 'left': {}, 'right': {}}\n","\n","        left_mask = data[:, best_feature] <= best_theta\n","        right_mask = ~left_mask\n","\n","        left_subtree = DecisionTreeRegressor(self.depth)\n","        left_subtree.fit(data[left_mask], target[left_mask], depth + 1)\n","        self.tree['left'] = left_subtree.tree\n","\n","        right_subtree = DecisionTreeRegressor(self.depth)\n","        right_subtree.fit(data[right_mask], target[right_mask], depth + 1)\n","        self.tree['right'] = right_subtree.tree\n","\n","\n","    def predict(self, X):\n","        # Method to handle multiple instances (rows in 2D array)\n","        if len(X.shape) != 2:\n","            raise ValueError(\"Predict method expects a 2D array of instances\")\n","        return np.array([self._predict_recursive(x, self.tree) for x in X])\n","\n","    def _predict_recursive(self, x, tree):\n","        # Internal method for recursive prediction for a single instance\n","        if 'value' in tree:\n","            return tree['value']\n","\n","        feature, theta = tree['feature'], tree['theta']\n","        if x[feature] <= theta:\n","            return self._predict_recursive(x, tree['left'])\n","        else:\n","            return self._predict_recursive(x, tree['right'])\n","\n","    # def _predict_recursive(self, x, node):\n","    #     # Internal method for recursive prediction for a single instance\n","    #     if node.value is not None:\n","    #         return node.value\n","\n","    #     if x[node.feature_index] <= node.threshold:\n","    #         return self._predict_recursive(x, node.left)\n","    #     else:\n","    #         return self._predict_recursive(x, node.right)\n","\n","def main():\n","    train_y, train_x = svm_read_problem(\"train.dat\")\n","    test_y, test_x = svm_read_problem(\"test.dat\")\n","    train_x = transform_X(train_x)\n","    test_x = transform_X(test_x)\n","\n","    train_x = np.array(train_x)\n","    train_y = np.array(train_y)\n","    test_x = np.array(test_x)\n","    test_y = np.array(test_y)\n","\n","    num_trees = 2000\n","    e_outs = []\n","\n","    for i in range(num_trees):\n","        # Bagging: Sample with replacement\n","        indices = np.random.choice(len(train_x), size=int(0.5 * len(train_x)), replace=True)\n","        bagged_x = train_x[indices]\n","        bagged_y = train_y[indices]\n","\n","        # Train model\n","        tree = DecisionTreeRegressor()\n","        tree.fit(bagged_x, bagged_y)\n","\n","        # Evaluate model\n","        predictions = tree.predict(test_x)\n","        squared_error = np.mean((test_y - predictions) ** 2)\n","        e_outs.append(squared_error)\n","        print(\"Random_Forest( {}/2000 )\".format(i+1))\n","\n","    # Plot histogram\n","    plt.hist(e_outs, bins=30, edgecolor='black')\n","    plt.xlabel('E_out')\n","    plt.ylabel('Frequency')\n","    plt.title('Histogram of E_out for 2000 Trees in Random Forest')\n","    plt.show()\n","\n","main()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dggaVYExXK6Z"},"outputs":[],"source":["# p11\n","\n","from libsvm.svmutil import *\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","\n","def transform_X(x):\n","  X = np.array([0,0,0,0,0,0,0,0])\n","  # X = np.empty(shape = (len(x), 8))\n","  for i in range(len(x)):\n","    dic = x[i]\n","    rows = [value for key,value in dic.items()]\n","    X = np.vstack([X, rows])\n","\n","  X = np.delete(X,0,0)\n","  return X\n","\n","\n","def compute_squared_error(y_true):\n","    return np.mean((y_true - np.mean(y_true)) ** 2)\n","\n","\n","def find_best_split(data, target):\n","    # Ensure data is a 2D numpy array and target is a 1D numpy array\n","    if len(data.shape) != 2 or len(target.shape) != 1:\n","        raise ValueError(\"Data must be a 2D array and target must be a 1D array\")\n","\n","    n_features = data.shape[1]\n","    best_feature, best_theta, min_error = None, None, float('inf')\n","\n","    for feature in range(n_features):\n","        sorted_values = np.unique(np.sort(data[:, feature]))\n","        thetas = (sorted_values[:-1] + sorted_values[1:]) / 2\n","\n","        for theta in thetas:\n","            left_mask = data[:, feature] <= theta\n","            right_mask = ~left_mask\n","\n","            left_target = target[left_mask]\n","            right_target = target[right_mask]\n","\n","            left_error = compute_squared_error(left_target) if left_target.size > 0 else 0\n","            right_error = compute_squared_error(right_target) if right_target.size > 0 else 0\n","\n","            total_error = left_error * left_mask.sum() + right_error * right_mask.sum()\n","\n","            if total_error < min_error:\n","                best_feature, best_theta, min_error = feature, theta, total_error\n","\n","    return best_feature, best_theta\n","\n","\n","class Node:\n","    def __init__(self, feature_index=None, threshold=None, left=None, right=None, value=None):\n","        self.feature_index = feature_index\n","        self.threshold = threshold\n","        self.left = left\n","        self.right = right\n","        self.value = value\n","\n","class DecisionTreeRegressor:\n","    def __init__(self, depth = None):\n","        self.depth = depth\n","        self.tree = {}\n","\n","    def fit(self, data, target, depth=0):\n","      # self.root = self.fit(data, target)\n","        if depth == self.depth or data.shape[0] < 2:\n","            self.tree['value'] = target.mean()\n","            return\n","\n","        best_feature, best_theta = find_best_split(data, target)\n","        if best_feature is None:\n","            self.tree['value'] = target.mean()\n","            return\n","\n","        self.tree = {'feature': best_feature, 'theta': best_theta, 'left': {}, 'right': {}}\n","\n","        left_mask = data[:, best_feature] <= best_theta\n","        right_mask = ~left_mask\n","\n","        left_subtree = DecisionTreeRegressor(self.depth)\n","        left_subtree.fit(data[left_mask], target[left_mask], depth + 1)\n","        self.tree['left'] = left_subtree.tree\n","\n","        right_subtree = DecisionTreeRegressor(self.depth)\n","        right_subtree.fit(data[right_mask], target[right_mask], depth + 1)\n","        self.tree['right'] = right_subtree.tree\n","\n","\n","    def predict(self, X):\n","        # Method to handle multiple instances (rows in 2D array)\n","        if len(X.shape) != 2:\n","            raise ValueError(\"Predict method expects a 2D array of instances\")\n","        return np.array([self._predict_recursive(x, self.tree) for x in X])\n","\n","    def _predict_recursive(self, x, tree):\n","        # Internal method for recursive prediction for a single instance\n","        if 'value' in tree:\n","            return tree['value']\n","\n","        feature, theta = tree['feature'], tree['theta']\n","        if x[feature] <= theta:\n","            return self._predict_recursive(x, tree['left'])\n","        else:\n","            return self._predict_recursive(x, tree['right'])\n","\n","    # def _predict_recursive(self, x, node):\n","    #     # Internal method for recursive prediction for a single instance\n","    #     if node.value is not None:\n","    #         return node.value\n","\n","    #     if x[node.feature_index] <= node.threshold:\n","    #         return self._predict_recursive(x, node.left)\n","    #     else:\n","    #         return self._predict_recursive(x, node.right)\n","\n","def main():\n","    train_y, train_x = svm_read_problem(\"train.dat\")\n","    test_y, test_x = svm_read_problem(\"test.dat\")\n","    train_x = transform_X(train_x)\n","    test_x = transform_X(test_x)\n","\n","    train_x = np.array(train_x)\n","    train_y = np.array(train_y)\n","    test_x = np.array(test_x)\n","    test_y = np.array(test_y)\n","\n","    num_trees = 2000\n","    e_in_trees = []\n","    e_out_trees = []\n","    trees = []\n","\n","    for i in range(num_trees):\n","        # Bagging: Sample with replacement\n","        indices = np.random.choice(len(train_x), size=int(0.5 * len(train_x)), replace=True)\n","        bagged_x = train_x[indices]\n","        bagged_y = train_y[indices]\n","\n","        # Train model\n","        tree = DecisionTreeRegressor()\n","        tree.fit(bagged_x, bagged_y)\n","        trees.append(tree)\n","\n","        # Evaluate model\n","        predictions_train = tree.predict(train_x)\n","        predictions_test = tree.predict(test_x)\n","\n","        e_in = np.mean((train_y - predictions_train) ** 2)\n","        e_out = np.mean((test_y - predictions_test) ** 2)\n","\n","        e_in_trees.append(e_in)\n","        e_out_trees.append(e_out)\n","        print(\"Random_Forest( {}/2000 )\".format(i+1))\n","\n","\n","    # Calculate E_in(G) and E_out(G)\n","    predictions_train_ensemble = np.mean([tree.predict(train_x) for tree in trees], axis=0)\n","    predictions_test_ensemble = np.mean([tree.predict(test_x) for tree in trees], axis=0)\n","\n","    e_in_ensemble = np.mean((train_y - predictions_train_ensemble) ** 2)\n","    e_out_ensemble = np.mean((test_y - predictions_test_ensemble) ** 2)\n","\n","    # Scatter plot of (Ein(gt), Eout(gt))\n","    plt.scatter(e_in_trees, e_out_trees, alpha=0.6, label='Individual Trees')\n","    plt.scatter(e_in_ensemble, e_out_ensemble, color='red', label='Random Forest', zorder=5)\n","    plt.xlabel('Ein(gt)')\n","    plt.ylabel('Eout(gt)')\n","    plt.title('Scatter Plot of Ein(gt) vs Eout(gt) for Random Forest')\n","    plt.legend()\n","    plt.show()\n","\n","main()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hftGtkEAYa01"},"outputs":[],"source":["# p12\n","\n","from libsvm.svmutil import *\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","\n","def transform_X(x):\n","  X = np.array([0,0,0,0,0,0,0,0])\n","  # X = np.empty(shape = (len(x), 8))\n","  for i in range(len(x)):\n","    dic = x[i]\n","    rows = [value for key,value in dic.items()]\n","    X = np.vstack([X, rows])\n","\n","  X = np.delete(X,0,0)\n","  return X\n","\n","\n","def compute_squared_error(y_true):\n","    return np.mean((y_true - np.mean(y_true)) ** 2)\n","\n","\n","def find_best_split(data, target):\n","    # Ensure data is a 2D numpy array and target is a 1D numpy array\n","    if len(data.shape) != 2 or len(target.shape) != 1:\n","        raise ValueError(\"Data must be a 2D array and target must be a 1D array\")\n","\n","    n_features = data.shape[1]\n","    best_feature, best_theta, min_error = None, None, float('inf')\n","\n","    for feature in range(n_features):\n","        sorted_values = np.unique(np.sort(data[:, feature]))\n","        thetas = (sorted_values[:-1] + sorted_values[1:]) / 2\n","\n","        for theta in thetas:\n","            left_mask = data[:, feature] <= theta\n","            right_mask = ~left_mask\n","\n","            left_target = target[left_mask]\n","            right_target = target[right_mask]\n","\n","            left_error = compute_squared_error(left_target) if left_target.size > 0 else 0\n","            right_error = compute_squared_error(right_target) if right_target.size > 0 else 0\n","\n","            total_error = left_error * left_mask.sum() + right_error * right_mask.sum()\n","\n","            if total_error < min_error:\n","                best_feature, best_theta, min_error = feature, theta, total_error\n","\n","    return best_feature, best_theta\n","\n","\n","class Node:\n","    def __init__(self, feature_index=None, threshold=None, left=None, right=None, value=None):\n","        self.feature_index = feature_index\n","        self.threshold = threshold\n","        self.left = left\n","        self.right = right\n","        self.value = value\n","\n","class DecisionTreeRegressor:\n","    def __init__(self, depth = None):\n","        self.depth = depth\n","        self.tree = {}\n","\n","    def fit(self, data, target, depth=0):\n","      # self.root = self.fit(data, target)\n","        if depth == self.depth or data.shape[0] < 2:\n","            self.tree['value'] = target.mean()\n","            return\n","\n","        best_feature, best_theta = find_best_split(data, target)\n","        if best_feature is None:\n","            self.tree['value'] = target.mean()\n","            return\n","\n","        self.tree = {'feature': best_feature, 'theta': best_theta, 'left': {}, 'right': {}}\n","\n","        left_mask = data[:, best_feature] <= best_theta\n","        right_mask = ~left_mask\n","\n","        left_subtree = DecisionTreeRegressor(self.depth)\n","        left_subtree.fit(data[left_mask], target[left_mask], depth + 1)\n","        self.tree['left'] = left_subtree.tree\n","\n","        right_subtree = DecisionTreeRegressor(self.depth)\n","        right_subtree.fit(data[right_mask], target[right_mask], depth + 1)\n","        self.tree['right'] = right_subtree.tree\n","\n","\n","    def predict(self, X):\n","        # Method to handle multiple instances (rows in 2D array)\n","        if len(X.shape) != 2:\n","            raise ValueError(\"Predict method expects a 2D array of instances\")\n","        return np.array([self._predict_recursive(x, self.tree) for x in X])\n","\n","    def _predict_recursive(self, x, tree):\n","        # Internal method for recursive prediction for a single instance\n","        if 'value' in tree:\n","            return tree['value']\n","\n","        feature, theta = tree['feature'], tree['theta']\n","        if x[feature] <= theta:\n","            return self._predict_recursive(x, tree['left'])\n","        else:\n","            return self._predict_recursive(x, tree['right'])\n","\n","    # def _predict_recursive(self, x, node):\n","    #     # Internal method for recursive prediction for a single instance\n","    #     if node.value is not None:\n","    #         return node.value\n","\n","    #     if x[node.feature_index] <= node.threshold:\n","    #         return self._predict_recursive(x, node.left)\n","    #     else:\n","    #         return self._predict_recursive(x, node.right)\n","\n","def main():\n","    train_y, train_x = svm_read_problem(\"train.dat\")\n","    test_y, test_x = svm_read_problem(\"test.dat\")\n","    train_x = transform_X(train_x)\n","    test_x = transform_X(test_x)\n","\n","    train_x = np.array(train_x)\n","    train_y = np.array(train_y)\n","    test_x = np.array(test_x)\n","    test_y = np.array(test_y)\n","\n","    num_trees = 2000\n","    e_out_individual_trees = []\n","    e_out_cumulative_forest = []\n","    cumulative_predictions = np.zeros_like(test_y, dtype=float)\n","\n","    for i in range(num_trees):\n","        # Bagging: Sample with replacement\n","        indices = np.random.choice(len(train_x), size=int(0.5 * len(train_x)), replace=True)\n","        bagged_x = train_x[indices]\n","        bagged_y = train_y[indices]\n","\n","        # Train model\n","        tree = DecisionTreeRegressor()\n","        tree.fit(bagged_x, bagged_y)\n","\n","        # Evaluate individual tree model\n","        predictions = tree.predict(test_x)\n","        squared_error = np.mean((test_y - predictions) ** 2)\n","        e_out_individual_trees.append(squared_error)\n","\n","        # Update cumulative predictions and calculate error for cumulative forest\n","        cumulative_predictions += predictions\n","        average_cumulative_predictions = cumulative_predictions / (i + 1)\n","        cumulative_forest_error = np.mean((test_y - average_cumulative_predictions) ** 2)\n","        e_out_cumulative_forest.append(cumulative_forest_error)\n","\n","        print(\"Random Forest Progress: {}/2000 Trees\".format(i + 1))\n","\n","    # Plotting\n","    plt.plot(range(1, num_trees + 1), e_out_individual_trees, label='E_out(gt)')\n","    plt.plot(range(1, num_trees + 1), e_out_cumulative_forest, label='E_out(Gt)')\n","    plt.xlabel('Number of Trees (t)')\n","    plt.ylabel('E_out')\n","    plt.title('E_out(gt) and E_out(Gt) as a Function of t')\n","    plt.legend()\n","    plt.show()\n","\n","main()\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNxV4K56DuPNGugvKzpuBWt","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.9.6"}},"nbformat":4,"nbformat_minor":0}
